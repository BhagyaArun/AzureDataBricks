{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c9e571-195d-4287-8f34-215b39239aeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ReadCSV\").getOrCreate()\n",
    "\n",
    "# Path to the CSV file in DBFS\n",
    "csv_file_path = \"/mnt/bhagi4c/dataset.csv\"  # Update this with your actual path\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5561cafc-b5b8-416f-92fb-cba3484c3838",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/bhagi4c has been unmounted.\n+--------+-------------+----------+-------+-----+--------------------+\n|order_id|customer_name|order_date|item_id|price|           item_name|\n+--------+-------------+----------+-------+-----+--------------------+\n|       1|     John Doe|2025-01-01|      3|  100|Apple iPhone 12 P...|\n|       1|     John Doe|2025-01-01|      7|  150|Apple MacBook Pro 16|\n|       2|   Jane Smith|2025-01-02|      1|   50|  Google Pixel 6 Pro|\n|       3|  Bob Johnson|2025-01-03|      5|   75|Samsung Galaxy Z ...|\n|       3|  Bob Johnson|2025-01-03|      9|  200|Beats Studio3 Wir...|\n|       4|  Emily Davis|2025-01-04|      2|   60|Apple iPhone 12 M...|\n|       5|Michael Brown|2025-01-05|      8|  170|Apple Watch Series 8|\n|       5|Michael Brown|2025-01-05|      4|   55|Apple iPhone 11, ...|\n+--------+-------------+----------+-------+-----+--------------------+\n\nroot\n |-- order_id: integer (nullable = true)\n |-- customer_name: string (nullable = true)\n |-- order_date: date (nullable = true)\n |-- item_id: integer (nullable = true)\n |-- price: integer (nullable = true)\n |-- item_name: string (nullable = true)\n\ndbfs:/mnt/bhagi4c/output_dataset/_SUCCESS\ndbfs:/mnt/bhagi4c/output_dataset/_committed_2641629759233796269\ndbfs:/mnt/bhagi4c/output_dataset/_started_2641629759233796269\ndbfs:/mnt/bhagi4c/output_dataset/part-00000-tid-2641629759233796269-a73b71c6-12a5-4279-99fa-3c9457c1bc21-68-1-c000.csv\ndbfs:/mnt/bhagi4c/output_dataset/part-00001-tid-2641629759233796269-a73b71c6-12a5-4279-99fa-3c9457c1bc21-69-1-c000.csv\ndbfs:/mnt/bhagi4c/output_dataset/part-00002-tid-2641629759233796269-a73b71c6-12a5-4279-99fa-3c9457c1bc21-70-1-c000.csv\ndbfs:/mnt/bhagi4c/output_dataset/part-00003-tid-2641629759233796269-a73b71c6-12a5-4279-99fa-3c9457c1bc21-71-1-c000.csv\n"
     ]
    }
   ],
   "source": [
    "# Set your storage account name and access key\n",
    "storage_account_name = \"bhagisa\"\n",
    "storage_account_access_key = \"IFKRUhlgpYcphuMYOMsh6IDXzmBdWI0C18s+W+9wbhg6vjEVC8h3x27xnz62oY1QUD6PWK52ivil+AStUu54Pg==\"\n",
    "\n",
    "# Configure access key\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\",\n",
    "  storage_account_access_key\n",
    ")\n",
    "\n",
    "# Mount Azure Blob Storage container to DBFS\n",
    "container_name = \"bhagi4c\"\n",
    "mount_point = f\"/mnt/{container_name}\"\n",
    "\n",
    "# Unmount if already mounted\n",
    "dbutils.fs.unmount(mount_point)\n",
    "\n",
    "# Mount the container\n",
    "dbutils.fs.mount(\n",
    "  source = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net\",\n",
    "  mount_point = mount_point,\n",
    "  extra_configs = {f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": storage_account_access_key}\n",
    ")\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = f\"/mnt/{container_name}/dataset.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# Function to get item details from the API\n",
    "def get_item_details(item_id):\n",
    "    url = f\"https://api.restful-api.dev/objects/{item_id}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for item ID {item_id}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "# Add item names to the dataset\n",
    "item_names = []\n",
    "for item_id in df.select('item_id').rdd.flatMap(lambda x: x).collect():\n",
    "    item_data = get_item_details(item_id)\n",
    "    if item_data and 'name' in item_data:\n",
    "        name = item_data['name']\n",
    "    else:\n",
    "        name = \"Unknown\"\n",
    "    item_names.append(name)\n",
    "\n",
    "# Convert the item_names list to a DataFrame\n",
    "item_names_df = pd.DataFrame(item_names, columns=[\"item_name\"])\n",
    "\n",
    "# Merge the item names with the original DataFrame\n",
    "df = df.toPandas()\n",
    "df[\"item_name\"] = item_names_df\n",
    "\n",
    "# Convert back to Spark DataFrame if needed\n",
    "df = spark.createDataFrame(df)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.show()\n",
    "\n",
    "# Display the schema\n",
    "df.printSchema()\n",
    "\n",
    "# Path to the output CSV file in Azure Blob Storage\n",
    "output_csv_file_path = f\"/mnt/{container_name}/output_dataset\"\n",
    "\n",
    "# Write the DataFrame directly to Azure Blob Storage\n",
    "df.write.csv(output_csv_file_path, header=True, mode='overwrite')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Product_list",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}